title: NPFL129, Lecture 10
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Gradient Boosting Decision Trees

## Milan Straka

### December 06, 2021

---
section: Gradient Boosting
# Gradient Boosting Decision Trees

The gradient boosting decision trees also train a collection of decision trees,
but unlike random forests, where the trees are trained independently,
in GBDT they are trained sequentially to correct the errors of the previous
trees.

~~~
![w=70%,f=right,mh=80%,v=middle](gbt_example.svgz)

If we denote $y_t$ as the prediction function of the $t^\mathrm{th}$
tree, the prediction of the whole collection is then
$$y(→x_i) = ∑_{t=1}^T y_t(→x_i; →W_t),$$
where $→W_t$ is a vector of parameters (leaf values, to be concrete) of the
$t^\mathrm{th}$ tree.

---
# Gradient Boosting for Regression

Considering a regression task first, we define the overall loss as

$$𝓛(⇉W) = ∑_i \ell\big(t_i, y(→x_i; ⇉W)\big) + ∑_{t=1}^T \frac{1}{2} λ \big\|⇉W_t\big\|^2,$$
where
~~~
- $⇉W = (→W_1, …, →W_T)$ are the parameters (leaf values) of the trees;

~~~
- $\ell\big(t_i, y(→x_i; ⇉W)\big)$ is an per-example loss, $(t_i - y(→x_i;
  ⇉W))^2$ for regression;
~~~
- the $λ$ is the usual $L_2$ regularization strength.

---
# Gradient Boosting for Regression

To construct the trees sequentially, we extend the definition to

$$𝓛^{(t)}(⇉W_t; →W_{1..t-1}) = ∑_i \Big[\ell\big(t_i, y^{(t-1)}(→x_i; ⇉W_{1..t-1}) + y_t(→x_i; →W_t)\big)\Big] + \frac{1}{2} λ \big\|⇉W_t\big\|^2.$$

~~~
In the following text, we drop the parameters of $y^{(t-1)}$ and $y_t$ for
brevity.

~~~
The original idea of gradient boosting was to set
$$y_t(→x_i) ∝ -\frac{∂\ell\big(t_i, y^{(t-1)}(→x_i)\big)}{∂y^{(t-1)}(→x_i)}$$
as a direction minimizing the residual loss
~~~
and then finding a suitable constant $γ_t$, which would minimize the loss
$$∑_i \Big[\ell\big(t_i, y^{(t-1)}(→x_i) + γ_t y_t(→x_i)\big)\Big] + \frac{1}{2} λ \big\|⇉W_t\big\|^2.$$

---
section: Newton’s Method
# First-order and Second-order Methods

Until now, we used mostly SGD for finding a minimum, by performing
$$→w ← →w - α ∇L(→w).$$

~~~
A disadvantage of this (so-called **first-order method**) is that we need to
specify the learning rates by ourselves, usually using quite a small one and
performing the update many times.

~~~
However, in some situations we can do better.

---
# Newton’s Root-Finding Method

Assume we got a function $f: ℝ → ℝ$ and we want to find its root. A SGD-like
algorithm would always move “towards” zero by taking small steps.

~~~
![w=40%,f=right](newton_iteration.svgz)

Instead, we could consider the a linear local approximation
(i.e., consider a line “touching” the function in a given point)
and perform a step so that our linear local approximation has
value 0:
$$x' ← x - \frac{f(x)}{f'(x)}.$$

~~~
## Finding Minima

The same method can be used to find minima, because a minimum
is just a root of a derivation, resulting in:
$$x' ← x - \frac{f'(x)}{f''(x)}.$$

---
# Newton’s Method

The following update is the Newton’s method of searching for extremes:
$$x' ← x - \frac{f'(x)}{f''(x)}.$$

It is a so-called **second-order** method, but it is in fact a SGD update
with learning rate $\frac{1}{f''(x)}$.

~~~
## Derivation from Taylor’s Expansion

The same update can be derived also from the Taylor’s expansion
$$f(x + ε) ≈ f(x) + ε f'(x) + \frac{1}{2} ε^2 f''(x), \textcolor{gray}{+ 𝓞(ε^3)}$$

~~~
which we can minimise for $ε$ by
$$0 = \frac{∂f(x + ε)}{∂ε} ≈ f'(x) + ε f''(x),\textrm{ ~obtaining~ }x + ε = x - \frac{f'(x)}{f''(x)}.$$

---
class: dbend
# Training NLPs with the Newton’s Method

Note that the second-order methods (methods utilizing second derivatives) are
impractical when training MLPs (and GLMs) with many parameters.
~~~
The problem is that there are too many second derivatives – if we consider
weights $→w ∈ ℝ^D$,
- the gradient $∇L(→w)$ has $D$ elements;
~~~
- however, we have a $D×D$ matrix with all second derivatives, called the
  **Hessian** $H$:
  $$H_{i,j} ≝ \frac{∂^2 L(→w)}{∂w_i ∂w_j}.$$

~~~
For completeness, the Taylor expansion than has the following form
$$f(→x + →ε) = f(→x) + →ε^T ∇f(→x) + \frac{1}{2} →ε^T H →ε,$$
from which we obtain the following second-order method update:
$$→x ← →x - H^{-1} ∇f(→x).$$

---
section: GB Training
# Gradient Boosting

However, a more principled approach was later suggested. Denoting
$$g_i = \frac{∂\ell\big(t_i, y^{(t-1)}(→x_i)\big)}{∂y^{(t-1)}(→x_i)}$$
and
$$h_i = \frac{∂^2\ell\big(t_i, y^{(t-1)}(→x_i)\big)}{∂y^{(t-1)}(→x_i)^2},$$

~~~
we can expand the objective $𝓛^{(t)}$ using a second-order approximation to
$$𝓛^{(t)}(⇉W_t; →W_{1..t-1}) ≈ ∑_i \Big[\ell\big(t_i, y^{(t-1)}(→x_i)\big) + g_i y_t(→x_i) + \frac{1}{2} h_i y_t^2(→x_i)\Big] + \frac{1}{2} λ \big\|⇉W_t\big\|^2.$$

---
# Gradient Boosting

Recall that we denote the indices of instances belonging to a node $𝓣$ as $I_𝓣$,
and let us denote the prediction for the node $𝓣$ as $w_𝓣$.
~~~
Then we can rewrite

$\displaystyle\kern2em 𝓛^{(t)}(⇉W_t; →W_{1..t-1})
 ≈ ∑\nolimits_i \Big[g_i y_t(→x_i) + \frac{1}{2} h_i y_t^2(→x_i)\Big] + \frac{1}{2} λ \big\|⇉W_t\big\|^2 + \textrm{const}$

~~~
$\displaystyle\kern2em\phantom{𝓛^{(t)}(⇉W_t; →W_{1..t-1})}
  ≈ ∑_𝓣 \bigg[\Big(∑_{i ∈ I_𝓣} g_i\Big) w_𝓣 + \frac{1}{2} \Big(λ + ∑_{i ∈ I_𝓣} h_i\Big) w_𝓣^2\bigg] + \textrm{const}.$

~~~
By setting a derivative with respect to $w_𝓣$ to zero, we get
$$0 = \frac{∂𝓛^{(t)}}{∂w_𝓣} = ∑\nolimits_{i ∈ I_𝓣} g_i + \Big(λ + ∑\nolimits_{i ∈ I_𝓣} h_i\Big) w_𝓣.$$

~~~
Therefore, the optimal weight for a node $𝓣$ is
$$w^*_𝓣 = -\frac{∑_{i ∈ I_𝓣} g_i}{λ + ∑_{i ∈ I_𝓣} h_i}.$$

---
# Gradient Boosting

Substituting the optimum weights to the loss, we get
$$𝓛^{(t)}(⇉W) ≈ -\frac{1}{2} ∑_𝓣 \frac{\left(∑_{i ∈ I_𝓣} g_i\right)^2}{λ + ∑_{i ∈ I_𝓣} h_i} + \textrm{const},$$

which can be used as a splitting criterion.

~~~
![w=60%,h=center](gbt_scores.svgz)

---
# Gradient Boosting

When splitting a node, the criterions of all possible splits can be effectively
computed using the following algorithm:

![w=60%,h=center](gbt_algorithm.svgz)

---
# Gradient Boosting

Furthermore, gradient boosting trees frequently use:
- data subsampling: either bagging, or (even more commonly) utilize only
  a fraction of the original training data for training a single tree (with 0.5
  being a common value),

~~~
- feature subsampling;
~~~
- shrinkage: multiply each trained tree by a learning rate $α$, which reduces
  influence of each individual tree and leaves space for future optimization.

---
section: GB Classification
# Binary Classification with Gradient Boosting Decision Trees

To perform classification, we train the trees to perform the linear part of
a generalized linear model.

~~~
Specifically, for a binary classification, we perform prediction by
$$σ\big(y(→x_i)\big) = σ\left(∑_{t=1}^T y_t(→x_i; →W_t)\right),$$
~~~
and the per-example loss is defined as
$$\ell\big(t_i, y(→x_i)\big) = -\log \Big[σ\big(y(→x_i)\big)^{t_i} \big(1- σ\big(y(→x_i)\big)\big)^{1-t_i}\Big].$$

---
# Multiclass Classification with Gradient Boosting Decision Trees

For multiclass classification, we need to model the full categorical output
distribution. Therefore, for each “timestep” $t$, we train $K$ trees $⇉W_{t,k}$,
each predicting a single value of the linear part of a generalized linear model.

~~~
Then, we perform prediction by
$$\softmax\big(→y(→x_i)\big) = \softmax\left(∑\nolimits_{t=1}^T y_{t,1}(→x_i; →W_{t,1}), …, ∑\nolimits_{t=1}^T y_{t,K}(→x_i; →W_{t,K})\right),$$
~~~
and the per-example loss is defined analogously as
$$\ell\big(t_i, →y(→x_i)\big) = -\log \Big(\softmax\big(→y(→x_i)\big)_{t_i}\Big).$$

---
# Multiclass Classification with Gradient Boosting Decision Trees

![w=80%,h=center](gradient_boosting_example_1.svgz)
~~~ ~
# Multiclass Classification with Gradient Boosting Decision Trees

![w=80%,h=center](gradient_boosting_example_2.svgz)
~~~ ~
# Multiclass Classification with Gradient Boosting Decision Trees

![w=80%,h=center](gradient_boosting_example_3.svgz)

---
section: GB Demo
# Gradient Boosting Demo and Implementations

## Playground

You can explore the [Gradient Boosting Trees playground](https://ufal.mff.cuni.cz/~straka/courses/npfl129/2122/slides/10/gbt/playground.html)
and [Gradient Boosting Trees explained](https://ufal.mff.cuni.cz/~straka/courses/npfl129/2122/slides/10/gbt/explained.html).

~~~
## Implementations

Scikit-learn offers an implementation of gradient boosting decision trees,
`sklearn.ensemble.GradientBoostingClassifier` for classification and
`sklearn.ensemble.GradientBoostingRegressor` for regression.

~~~
- Furthermore, the `sklearn.ensemble.HistGradientBoosting{Classifier/Regressor}`
  provides histogram-based splitting (which can be much faster for larger
  datasets – tens of thousands of examples and more) and efficient categorical
  feature splitting.

~~~
There are additional efficient implementations, capable of distributed
processing of data larger than available memory (both offering also scikit-learn
interface):
- XGBoost,
- LightGBM (which is the inspiration for the `HistGradientBoosting*` implementation).

---
section: SupervisedML
# Supervised Machine Learning

This concludes the **supervised machine learning** part of our course.

~~~
We have encountered:
- parametric models

~~~
  - generalized linear models: perceptron algorithm, linear regression, logistic regression,
    multinomial (softmax) logistic regression, Poisson regression
    - linear models, but manual feature engineering allows solving
      non-linear problems
~~~
  - multilayer perceptron: non-linear, perfect approximator – Universal approx. theorem
~~~

- non-parametric models
  - k-nearest neighbors
~~~
  - kernelized linear regression
~~~
  - support vector machines

~~~
- decision trees
  - can be both parametric or non-parametric depending on the constraints

~~~
- generative models
  - naive Bayes

---
# Supervised Machine Learning

When training a model for a new dataset, I start by evaluating two models:

~~~
- an **MLP** with one/two hidden layers
~~~
  - works best for high-dimensional data (images, speech, text), where an
    individual single dimension (feature) does not convey much meaning;
~~~
- **gradient boosted decision tree**
~~~
  - works best for lower-dimensional data, where the input features have
    interpretation on their own.

~~~
However, if the amount of training examples is not too large (tens/hundreds of thousands
at most) and there are lot of features, **SVM** with **RBF** kernel might offer
best performance.

~~~
Furthermore, if there are only a few training examples with a lot of features,
**naive Bayes** might also work well.

~~~
Finally, if your goal is to reach the highest possible performance and you have
a lot of resources, definitely use **ensembling**.
