title: NPFL129, Lecture 12
class: title, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Gaussian Mixture

## Milan Straka

### December 20, 2021

---
# K-Means Clustering

<div class="algorithm">

**Input**: Input points $→x_1, …, →x_N$, number of clusters $K$.

- Initialize $→μ_1, …, →μ_K$ as $K$ random input points.

- Repeat until convergence (or until patience runs out):
  - Compute the best possible $z_{i, k}$. It is easy to see that the smallest $J$
    is achieved by
    $$z_{i,k} = \begin{cases} 1 & \textrm{~~if~}k = \argmin\nolimits_j \|→x_i - →μ_j\|^2, \\
                              0 & \textrm{~~otherwise}.\end{cases}$$

  - Compute the best possible $→μ_k = \argmin\nolimits_{→μ} ∑_i z_{i,k} \|→x_i-→μ\|^2$.
   By computing a derivative with respect to $→μ$, we get
   $$→μ_k = \frac{∑_i z_{i,k} →x_i}{∑_i z_{i,k}}.$$
</div>

---
# K-Means Clustering

![w=55%,h=center](../11/kmeans_example.svgz)

---
# Gaussian Mixture vs K-Means

It could be useful to consider that different clusters might have different
radii or even be ellipsoidal.

![w=100%,h=center](../11/mog_kmeans_comparison.svgz)

---
section: MultivariateGaussian
# Multivariate Gaussian Distribution

Recall that
$$𝓝(x; μ, σ^2) = \sqrt{\frac{1}{2πσ^2}} \exp \left(-\frac{(x - μ)^2}{2σ^2}\right).$$

~~~
For $D$-dimensional vector $→x$, the multivariate Gaussian distribution takes
the form
$$𝓝(→x | →μ, ⇉Σ) ≝ \frac{1}{\sqrt{(2π)^D |Σ|}} \exp \left(-\frac{1}{2}(→x-→μ)^T ⇉Σ^{-1} (→x-→μ) \right).$$

~~~
The biggest difference compared to the single-dimensional case is the _covariance
matrix_ $⇉Σ$, which is (in the non-degenerate case, which is the only one
considered here) a _symmetric positive-definite matrix_ of size $D × D$.

---
# Multivariate Gaussian Distribution

If the covariance matrix is an identity, then the multivariate Gaussian
distribution simplifies to
$$𝓝(→x | →μ, ⇉I) = \frac{1}{\sqrt{(2π)^D}} \exp \left(-\frac{1}{2}(→x - →μ)^T (→x - →μ)\right).$$

~~~
![w=45%,f=right](multivariate_gaussian_plot_eye.svgz)

We can rewrite the exponent in this case to
$$𝓝(→x | →μ, ⇉I) ∝ \exp \left(-\frac{\|→x - →μ\|^2}{2}\right).$$

Therefore, the constant surfaces are concentric circles centered at the mean $→μ$.

~~~
The same holds if the covariance is $σ^2 ⇉I$, only the circles' diameter
changes.

---
# Multivariate Gaussian Distribution

Now consider a diagonal covariance matrix $⇉Λ$. The exponent then simplifies to

$$𝓝(→x | →μ, ⇉Λ) ∝ \exp \left(- ∑\nolimits_i \frac{1}{2⇉Λ_{i,i}} \big(→x_i - →μ_i\big)^2\right).$$

~~~
![w=45%,f=right](multivariate_gaussian_plot_diag.svgz)

The constant surfaces in this case are axis-aligned ellipses centered at the
mean $→μ$ with size of the axes depending on the corresponding diagonal entries
in the covariance matrix.

---
# Multivariate Gaussian Distribution

In the general case of a full covariance matrix, the fact that it is positive
definite implies it has real positive _eigenvalues_ $λ_i$. Considering the
corresponding eigenvectors $→u_i$, it can be shown that the constant
surfaces are again ellipses centered at $→μ$, but this time rotated so that
their axes are the eigenvectors $→u_i$ with sizes $λ_i^{1/2}$.

![w=48%](multivariate_gaussian_elipsoids.svgz)![w=88%,mw=50%,h=center](multivariate_gaussian_plot_full.svgz)

---
# Multivariate Gaussian Distribution

Generally, we can rewrite a positive-definite matrix $⇉Σ$ as $(⇉U⇉Λ^{1/2})(⇉U⇉Λ^{1/2})^T$,
and then
$$→x ∼ 𝓝(→μ, ⇉Σ) \iff →x ∼ →μ + ⇉U⇉Λ^{1/2} 𝓝(0, ⇉I).$$

~~~
Therefore, when sampling from a distribution with a full covariance matrix, we
can sample from a standard multivariate $𝓝(0, ⇉I)$, scale by the eigenvalues of
the covariance matrix, rotate according to the eigenvectors of the covariance
matrix and finally shifting by $→μ$.

~~~
![w=48%,f=right](multivariate_gaussian_covariance.svgz)

Note that different forms of covariance allows more generality, but also
requires more parameters:
- the $σ^2 ⇉I$ has a single parameter,
- the $⇉Λ$ has $D$ parameters,
- the full covariance matrix $⇉Σ$ has $\binom{D+1}{2}$ parameters, i.e., $Θ(D^2)$.

---
section: GaussianMixture
# Gaussian Mixture

Let $→x_1, →x_2, …, →x_N$ be a collection of $N$ input examples, each being
a $D$-dimensional vector $→x_i ∈ ℝ^D$. Let $K$, the number of target clusters,
be given.

~~~
Our goal is to represent the data as a Gaussian mixture, which is a combination
of $K$ Gaussians in the form
$$p(→x) = ∑_{k=1}^K π_k 𝓝(→x | →μ_k, ⇉Σ_k).$$
Therefore, each cluster is parametrized as $𝓝(→x | →μ_k, ⇉Σ_k)$.

---
# Gaussian Mixture

![w=75%,h=center](mog_data.svgz)
![w=75%,h=center](mog_illustration.svgz)

---
section: MoGClust
# Gaussian Mixture

Let $→x_1, →x_2, …, →x_N$ be a collection of $N$ input examples, each being
a $D$-dimensional vector $→x_i ∈ ℝ^D$. Let $K$, the number of target clusters,
be given.

Our goal is to represent the data as a Gaussian mixture, which is a combination
of $K$ Gaussians in the form
$$p(→x) = ∑_{k=1}^K π_k 𝓝(→x | →μ_k, ⇉Σ_k).$$
Therefore, each cluster is parametrized as $𝓝(→x | →μ_k, ⇉Σ_k)$.

~~~
Let $→z ∈ \{0, 1\}^K$ be a $K$-dimensional random variable, such that exactly
one $z_k$ is 1, denoting to which cluster a training example belongs.
~~~
Let the marginal distribution of $z_k$ be
$$p(z_k = 1) = π_k,$$
so that the priors $π_k$ represent the “fertility” of the clusters.
~~~
Then, $p(→z) = ∏_k π_k^{z_k}$.

---
# Gaussian Mixture

![w=6%,f=right](mog_latent.svgz)

We can write
$$p(→x) = ∑_{→z} p(→z) p(→x | →z) = ∑_{k=1}^K π_k 𝓝(→x | →μ_k, ⇉Σ_k),$$
~~~
and the probability of the whole clustering is therefore
$$\log p(⇉X | →π, →μ, →Σ) = ∑_{i=1}^N \log \left(∑_{k=1}^K π_k 𝓝(→x_i | →μ_k, ⇉Σ_k)\right).$$

~~~
To fit a Gaussian mixture model, we utilize maximum likelihood estimation and
minimize
$$𝓛(⇉X) = - ∑_i \log ∑_{k=1}^K π_k 𝓝(→x_i | →μ_k, ⇉Σ_k).$$

---
# Gaussian Mixture

The derivative of the loss with respect to $→μ_k$ gives
$$\frac{∂𝓛(⇉X)}{∂→μ_k} = - ∑_i \frac{π_k 𝓝(→x_i | →μ_k, ⇉Σ_k)}{∑_{l=1}^K π_l 𝓝(→x_i | →μ_l, ⇉Σ_l)} ⇉Σ_k^{-1} \big(→x_i - →μ_k\big).$$

~~~
Denoting $r(z_{i,k}) = \frac{π_k 𝓝(→x_i | →μ_k, ⇉Σ_k)}{∑_{l=1}^K π_l 𝓝(→x_i | →μ_l, ⇉Σ_l)}$,
setting the derivative equal to zero and multiplying by $⇉Σ_k^{-1}$, we get
$$→μ_k = \frac{∑_i r(z_{i,k}) →x_i}{∑_i r(z_{i,k})}.$$

~~~
The $r(z_{i,k})$ are usually called **responsibilities** and denote the
probability $p(z_k = 1 | →x_i)$. Note that the responsibilities depend on
$→μ_k$, so the above equation is not an analytical solution for $→μ_k$, but
can be used as an _iterative_ algorithm for converging to a local optimum.

---
# Gaussian Mixture

For $⇉Σ_k$, we again compute the derivative of the loss, which is technically
complicated (we need to compute a derivative of a matrix inverse, and also we
need to differentiate matrix determinant) and results in an analogous equation:
$$→Σ_k = \frac{∑_i r(z_{i,k}) (→x_i - →μ_k) (→x_i - →μ_k)^T}{∑_i r(z_{i,k})}.$$

~~~
To minimize the loss with respect to $→π$, we need to include the constraint
$∑_k π_k = 1$, so we form a Lagrangian $𝓛(⇉X) - λ\left(∑\nolimits_k π_k - 1\right)$,
and get
$$\frac{∂𝓛(⇉X)}{∂π_k} = ∑_i \frac{𝓝(→x_i | →μ_k, ⇉Σ_k)}{∑_{l=1}^K π_l 𝓝(→x_i | →μ_l, ⇉Σ_l)} - λ.$$
~~~
Setting the derivative to zero and multiplying it by $π_k$, we obtain $π_k = \frac{1}{λ} ⋅ ∑_i r(z_{i,k})$, so
$$π_k = 1/N ⋅ ∑\nolimits_i r(z_{i,k}).$$

---
# Gaussian Mixture

<div class="algorithm">

**Input**: Input points $→x_1, …, →x_N$, number of clusters $K$.

- Initialize $→μ_k, ⇉Σ_k$ and $π_k$. It is common to start by running the
  K-Means algorithm to obtain $z_{i,k}$, set $r(z_{i,k}) ← z_{i,k}$
  and use the **M step** below.

~~~
- Repeat until convergence (or until patience runs out):
  - **E step**. Evaluate the responsibilities as
    $$r(z_{i,k}) = \frac{π_k 𝓝(→x_i | →μ_k, ⇉Σ_k)}{∑_{l=1}^K π_l 𝓝(→x_i | →μ_l, ⇉Σ_l)}.$$
~~~
  - **M step**. Maximize the log-likelihood by setting

    $$→μ_k = \frac{∑_i r(z_{i,k}) →x_i}{∑_i r(z_{i,k})},~~
      →Σ_k = \frac{∑_i r(z_{i,k}) (→x_i - →μ_k) (→x_i - →μ_k)^T}{∑_i r(z_{i,k})},~~
      π_k = \frac{∑_i r(z_{i,k})}{N}.$$

</div>

---
# Gaussian Mixture

![w=75%,h=center](mog_example.svgz)

